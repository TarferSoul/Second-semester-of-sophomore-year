{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:03:59.276148900Z",
     "start_time": "2024-05-22T09:03:59.264641400Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "from docx import Document"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T09:27:48.657469200Z",
     "start_time": "2024-05-27T09:27:48.490246100Z"
    }
   },
   "id": "147eb0a966a35019",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def read_docx(file_path):\n",
    "    \"\"\"读取DOCX文件并返回其中的文本内容\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    text = []\n",
    "    for para in doc.paragraphs:\n",
    "        text.append(para.text)\n",
    "    return ''.join(text)\n",
    "\n",
    "def collect_statistics(text):\n",
    "    \"\"\"统计字母、标点符号和空格的频率\"\"\"\n",
    "    char_count = Counter(text)\n",
    "    total_chars = sum(char_count.values())\n",
    "    return char_count, total_chars"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T09:38:54.946301400Z",
     "start_time": "2024-05-27T09:38:54.931273900Z"
    }
   },
   "id": "147f2712de1357dc",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_entropy(filtered_text, order=0):\n",
    "    \"\"\"计算文本的熵值，使用不同阶的马尔可夫模型\"\"\"\n",
    "    \n",
    "    if order == 0:\n",
    "        # 0阶马尔可夫模型：每个字符独立选择\n",
    "        char_count = Counter(filtered_text)\n",
    "        total_chars = sum(char_count.values())\n",
    "        entropy = 0\n",
    "        for count in char_count.values():\n",
    "            probability = count / total_chars\n",
    "            entropy -= probability * math.log2(probability)\n",
    "        return entropy\n",
    "    else:\n",
    "        # 高阶马尔可夫模型\n",
    "        storage = defaultdict(Counter)\n",
    "        total_ngrams = 0\n",
    "\n",
    "        for i in range(len(filtered_text) - order):\n",
    "            prefix = filtered_text[i:i + order]\n",
    "            next_char = filtered_text[i + order]\n",
    "            storage[prefix][next_char] += 1\n",
    "            total_ngrams += 1\n",
    "\n",
    "        entropy = 0\n",
    "        for prefix, suffix_counts in storage.items():\n",
    "            prefix_total = sum(suffix_counts.values())\n",
    "            for count in suffix_counts.values():\n",
    "                probability = count / prefix_total\n",
    "                entropy -= (prefix_total / total_ngrams) * probability * math.log2(probability)\n",
    "\n",
    "        return entropy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T09:41:33.857934700Z",
     "start_time": "2024-05-27T09:41:33.845423500Z"
    }
   },
   "id": "88c1352277c883e8",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def markov_model(text, order):\n",
    "    \"\"\"构建马尔可夫模型并输出模型状态和转移概率\"\"\"\n",
    "    filtered_text = text.lower()\n",
    "    ngrams = defaultdict(Counter)\n",
    "\n",
    "    for i in range(len(filtered_text) - order):\n",
    "        prefix = filtered_text[i:i + order]\n",
    "        next_char = filtered_text[i + order]\n",
    "        ngrams[prefix][next_char] += 1\n",
    "\n",
    "    total_ngrams = sum(sum(suffix_counts.values()) for suffix_counts in ngrams.values())\n",
    "    alphabet = sorted(set(filtered_text))\n",
    "    alphabet_size = len(alphabet)\n",
    "    model_matrix = np.zeros((alphabet_size ** order, alphabet_size))\n",
    "\n",
    "    char_to_index = {char: index for index, char in enumerate(alphabet)}\n",
    "\n",
    "    for prefix, suffix_counts in ngrams.items():\n",
    "        prefix_index = sum(char_to_index[char] * (alphabet_size ** (order - i - 1)) for i, char in enumerate(prefix))\n",
    "        prefix_total = sum(suffix_counts.values())\n",
    "        for char, count in suffix_counts.items():\n",
    "            char_index = char_to_index[char]\n",
    "            model_matrix[prefix_index, char_index] = count / prefix_total\n",
    "\n",
    "    model = {}\n",
    "    for prefix, suffix_counts in ngrams.items():\n",
    "        prefix_total = sum(suffix_counts.values())\n",
    "        model[prefix] = {char: count / prefix_total for char, count in suffix_counts.items()}\n",
    "\n",
    "    return model, total_ngrams, model_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T09:44:59.024481300Z",
     "start_time": "2024-05-27T09:44:59.015369300Z"
    }
   },
   "id": "22c7ec6c469d221e",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_markov_model(model):\n",
    "    \"\"\"打印马尔可夫模型的状态和转移概率\"\"\"\n",
    "    print(model)\n",
    "    for prefix, transitions in model.items():\n",
    "        print(f\"State: {prefix}\")\n",
    "        for char, prob in transitions.items():\n",
    "            print(f\"  {char}: {prob:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T09:44:59.538279900Z",
     "start_time": "2024-05-27T09:44:59.518611600Z"
    }
   },
   "id": "c6eee6a7f2475527",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk Speech Statistics:\n",
      "Counter({' ': 1412, 'e': 697, 't': 660, 'a': 525, 'o': 500, 'n': 418, 'i': 382, 's': 349, 'h': 345, 'r': 284, 'l': 258, 'd': 214, 'c': 201, 'u': 183, 'w': 157, 'g': 149, 'y': 137, 'f': 133, 'm': 128, 'p': 103, ',': 91, '.': 91, 'b': 79, 'k': 63, '’': 55, 'I': 54, 'v': 53, 'A': 24, 'S': 22, 'B': 16, 'T': 16, 'P': 13, '-': 9, 'C': 7, '?': 6, 'M': 5, 'x': 5, '1': 4, 'W': 4, 'q': 4, '0': 4, '–': 4, 'O': 4, '—': 3, '2': 3, 'z': 3, '‘': 3, '9': 3, 'F': 3, 'Y': 3, 'X': 3, 'R': 3, 'E': 2, 'L': 2, 'N': 2, 'G': 2, 'D': 2, '“': 1, '3': 1, '5': 1, '8': 1, 'j': 1, '4': 1, ':': 1, '”': 1})\n"
     ]
    }
   ],
   "source": [
    "Text_file = 'Elon_Musk_Speech.docx'\n",
    "Text = read_docx(Text_file)\n",
    "\n",
    "Text_stats, elon_total = collect_statistics(Text)\n",
    "print(\"Elon Musk Speech Statistics:\")\n",
    "print(Text_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T09:44:59.816406100Z",
     "start_time": "2024-05-27T09:44:59.795612100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Entropy (0th order): 4.385606450470013\n",
      "Text Entropy (3rd order): 1.188728794461539\n",
      "Text Entropy (5th order): 0.4271118482058984\n"
     ]
    }
   ],
   "source": [
    "Text_entropy_0 = calculate_entropy(Text, 0)\n",
    "Text_entropy_3 = calculate_entropy(Text, 3)\n",
    "Text_entropy_5 = calculate_entropy(Text, 5)\n",
    "print(f\"\\nText Entropy (0th order): {Text_entropy_0}\")\n",
    "print(f\"Text Entropy (3rd order): {Text_entropy_3}\")\n",
    "print(f\"Text Entropy (5th order): {Text_entropy_5}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T09:45:00.409780300Z",
     "start_time": "2024-05-27T09:45:00.376568300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对于这个结果，可以认为0-1阶的Markov模型近似满足真实的英文熵值。\n",
    "而由于所提供的英文文本内容有限，所以对于更高阶的Markov模型，得到的熵值不会趋于值约为1.4的极限。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_stationary_distribution(P):\n",
    "    \"\"\"计算马尔可夫链的平稳分布\"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eig(P.T)\n",
    "    stationary_index = np.argmin(np.abs(eigvals - 1.0))\n",
    "    stationary_distribution = np.real(eigvecs[:, stationary_index])\n",
    "    stationary_distribution /= np.sum(stationary_distribution)\n",
    "    return stationary_distribution\n",
    "\n",
    "def calculate_entropy_rate(P, pi):\n",
    "    \"\"\"计算马尔可夫链的熵率\"\"\"\n",
    "    H = 0\n",
    "    for i in range(P.shape[0]):\n",
    "        for j in range(P.shape[1]):\n",
    "            if P[i, j] > 0:\n",
    "                H -= pi[i] * P[i, j] * np.log2(P[i, j])\n",
    "    return H"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T09:49:51.851008600Z",
     "start_time": "2024-05-27T09:49:51.842498200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elon Musk Speech Markov Model (Order 5):\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 70.6 GiB for an array with shape (205962976, 46) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m order \u001B[38;5;129;01min\u001B[39;00m orders:\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mElon Musk Speech Markov Model (Order \u001B[39m\u001B[38;5;132;01m{\u001B[39;00morder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m):\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m     elon_model, elon_total_ngrams, model_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mmarkov_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mText\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     print_markov_model(elon_model)\n\u001B[0;32m      6\u001B[0m     pi \u001B[38;5;241m=\u001B[39m calculate_stationary_distribution(model_matrix)\n",
      "Cell \u001B[1;32mIn[27], line 14\u001B[0m, in \u001B[0;36mmarkov_model\u001B[1;34m(text, order)\u001B[0m\n\u001B[0;32m     12\u001B[0m alphabet \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(\u001B[38;5;28mset\u001B[39m(filtered_text))\n\u001B[0;32m     13\u001B[0m alphabet_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(alphabet)\n\u001B[1;32m---> 14\u001B[0m model_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43malphabet_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malphabet_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m char_to_index \u001B[38;5;241m=\u001B[39m {char: index \u001B[38;5;28;01mfor\u001B[39;00m index, char \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(alphabet)}\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prefix, suffix_counts \u001B[38;5;129;01min\u001B[39;00m ngrams\u001B[38;5;241m.\u001B[39mitems():\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 70.6 GiB for an array with shape (205962976, 46) and data type float64"
     ]
    }
   ],
   "source": [
    "orders = [5]\n",
    "for order in orders:\n",
    "    print(f\"\\nElon Musk Speech Markov Model (Order {order}):\")\n",
    "    elon_model, elon_total_ngrams, model_matrix = markov_model(Text, order)\n",
    "    print_markov_model(elon_model)\n",
    "    pi = calculate_stationary_distribution(model_matrix)\n",
    "    entropy_rate = calculate_entropy_rate(model_matrix, pi)\n",
    "\n",
    "    print(\"平稳分布:\")\n",
    "    print(pi)\n",
    "    print(\"\\n熵率:\")\n",
    "    print(entropy_rate)\n",
    "\n",
    "    with open(f'Elon Musk Speech Markov Model (Order {order}).txt', 'w', encoding='utf-8',) as f:\n",
    "        f.write(str(elon_model))\n",
    "        f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T09:53:19.390892Z",
     "start_time": "2024-05-27T09:53:19.351443600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面是喂了10亿单词的语料库"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "def read_text_files(folder_path):\n",
    "    english_texts = []\n",
    "    # 遍历文件夹下所有的.txt文件\n",
    "    for file_path in glob.glob(os.path.join(folder_path, '*.txt')):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # 读取文件内容\n",
    "            text = file.read()\n",
    "            # 提取英文内容（假设只包含字母、空格和换行符）\n",
    "            english_text = ''.join(filter(lambda x: x.isalpha() or x.isspace(), text))\n",
    "            english_texts.append(english_text)\n",
    "    return str(english_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:03:59.501450900Z",
     "start_time": "2024-05-22T09:03:59.463801500Z"
    }
   },
   "id": "7153e0f7d66a9485",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "english_texts = read_text_files('text0/')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:04:08.142890300Z",
     "start_time": "2024-05-22T09:03:59.478423200Z"
    }
   },
   "id": "ccb3b5ee0dfeaeda",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order 0 Markov Model, Entropy: 4.254037364372333\n",
      "Order 3 Markov Model, Entropy: 2.3822194443288214\n",
      "Order 5 Markov Model, Entropy: 1.7797140240443352\n"
     ]
    }
   ],
   "source": [
    "for order in [0,3,5]:  # 计算0阶到5阶马尔可夫模型的熵值\n",
    "    entropy = calculate_entropy(english_texts, order)\n",
    "    print(f'Order {order} Markov Model, Entropy: {entropy}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:05:30.773938100Z",
     "start_time": "2024-05-22T09:04:08.142890300Z"
    }
   },
   "id": "3a645f0af1745b45",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:05:30.794097200Z",
     "start_time": "2024-05-22T09:05:30.775025500Z"
    }
   },
   "id": "39ff67fb3b6a257b",
   "execution_count": 70
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
